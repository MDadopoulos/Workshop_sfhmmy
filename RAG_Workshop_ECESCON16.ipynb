{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "5-f8-Il-O2o-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries"
      ],
      "metadata": {
        "id": "iM9-CcQqNJeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community tiktoken langchainhub langchain langchain-google-genai\n",
        "! pip install -qU langchain-huggingface sentence_transformers chromadb  langchain-qdrant qdrant_client"
      ],
      "metadata": {
        "id": "9uKGCugsp_a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insert API key\n"
      ],
      "metadata": {
        "id": "1Y-7Ab8ANgQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need first to get an API key from [Google AI Studio](https://aistudio.google.com/app/apikey)."
      ],
      "metadata": {
        "id": "NDJTzUjpPbfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY = \"\"  # add your GOOGLE API key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "#or run this\n",
        "#from google.colab import userdata\n",
        "#from google.colab import drive\n",
        "#os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "5OFp3k9gOny0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing"
      ],
      "metadata": {
        "id": "xue1q_JwNTRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Load Data"
      ],
      "metadata": {
        "id": "CJXBK2WeNA6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From your files"
      ],
      "metadata": {
        "id": "91fbe_FeVkJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured\n",
        "!pip install \"unstructured[pdf]\""
      ],
      "metadata": {
        "id": "T03YzOXPuLhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create the folder\n",
        "folder_path = os.path.join(\"/content/\", \"uploaded_files\")\n",
        "os.makedirs(folder_path, exist_ok=True)  # Create if it doesn't exist\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to the folder\n",
        "for filename, data in uploaded.items():\n",
        "  source_path = os.path.join(\"/content/\", filename)  # Path to uploaded file\n",
        "  destination_path = os.path.join(folder_path, filename)\n",
        "  shutil.move(source_path, destination_path)  # Move the file"
      ],
      "metadata": {
        "id": "e_tSe5C-YOX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader(folder_path)\n",
        "docs = loader.load()\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "LJF6Ekt6ybKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2551901-1fbf-43cd-8eb1-523dfb013a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P2' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P2' is an invalid float value\n",
            "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'P1' is an invalid float value\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From URLs"
      ],
      "metadata": {
        "id": "NRNXp47KVoB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Load Documents\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "3sB0fn_zXfoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f1c356-6c36-43fa-ad07-fe7e68a36812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "K0JZNettypAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd29501-b538-4e64-9d37-4f1a887dfdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Use a Text Splitter to Split Documents"
      ],
      "metadata": {
        "id": "jrOYMSUVbU3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "gOK-K7cNaoNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjpNB0r6u9St",
        "outputId": "bf736683-677e-4839-908b-d022a90b1a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "241"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embed the documents and store them"
      ],
      "metadata": {
        "id": "0KRuEqoab-_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "#sentence-transformers/all-MiniLM-L6-v2\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        "\n",
        ")\n",
        "print(f\"Model's maximum sequence length: {SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').max_seq_length}\")"
      ],
      "metadata": {
        "id": "fNeCHPd2biMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "client = QdrantClient(\n",
        "    path=\"/content/vector_store_folder\"\n",
        "    #\":memory:\"\n",
        "    # you can use :memory: mode for fast and light-weight experiments,\n",
        ")\n",
        "client.create_collection(\n",
        "    collection_name=\"workshop_collection\",\n",
        "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"workshop_collection\",\n",
        "    embedding=embedding_model\n",
        ")\n",
        "vector_store.add_documents(documents=splits)\n"
      ],
      "metadata": {
        "id": "ZrXMzP0NjiP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In case qdrant doesnt work, use this:"
      ],
      "metadata": {
        "id": "v7Qj_xj-rv8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=embedding_model)"
      ],
      "metadata": {
        "id": "4Ejaa4S0gMLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval"
      ],
      "metadata": {
        "id": "XjiRMs8_sHy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query= \"What are the three core parts of an agent?\" #\"Which system became the first AI to earn an IMO medal?\""
      ],
      "metadata": {
        "id": "VLXUmPOfxy1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = vector_store.similarity_search_with_score(query=user_query, k=5)"
      ],
      "metadata": {
        "id": "FyIyV99OhyQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMTWq_V19nNX",
        "outputId": "e3f4640f-17c5-4287-a0a2-e0ba7dfc7784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '2ba4a80cad7c4ddeaafab4d6122e52c4', '_collection_name': 'workshop_collection'}, page_content='Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.'),\n",
              "  0.5085810586457418),\n",
              " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '3d05827017bc439abe5156624b04d63b', '_collection_name': 'workshop_collection'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
              "  0.4987262613833804),\n",
              " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '9a30f9c352af44eeade5b1b372ad651c', '_collection_name': 'workshop_collection'}, page_content='Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)'),\n",
              "  0.47357487553006467),\n",
              " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '89743c9836b546a6a09c2ff531233b3f', '_collection_name': 'workshop_collection'}, page_content='Agent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:'),\n",
              "  0.4725851512916192),\n",
              " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '21f57106ae8a4ac489f51752112be5b0', '_collection_name': 'workshop_collection'}, page_content='This benchmark evaluates the agent’s tool use capabilities at three levels:'),\n",
              "  0.4633498657703144)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Content:\",retrieved_docs[0][0].page_content)\n",
        "print(\"Metadata\",retrieved_docs[0][0].metadata)\n",
        "print(\"Similarity score\",retrieved_docs[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwuklVLs9lZN",
        "outputId": "afac9de0-9e4c-414d-f90e-27d90fba2180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content: Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
            "Environment information is present in a tree structure.\n",
            "Metadata {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '2ba4a80cad7c4ddeaafab4d6122e52c4', '_collection_name': 'workshop_collection'}\n",
            "Similarity score 0.5085810586457418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Retriever"
      ],
      "metadata": {
        "id": "m0uvNSzgOzbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "docs = retriever.get_relevant_documents(user_query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myjSzHe0iKOB",
        "outputId": "118e1cae-bf6b-4c16-8002-06a9853ac4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-c35d213439dc>:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(user_query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
            "Environment information is present in a tree structure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "Lz_ocOQjO5fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define prompt"
      ],
      "metadata": {
        "id": "rMAMVbulPFzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# Prompt\n",
        "\n",
        "template = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.If the provided context doesn't contain the answer, answer from your knowledge but say that you do,else just say that you don't know, don't try to make up an answer.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "Zr_a-ocTymHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure LLM"
      ],
      "metadata": {
        "id": "n5xEkTPJPIif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-lite\",#\"gemini-2.0-flash\",#\"gemini-2.5-flash-preview-04-17\",#\n",
        "    temperature=0,\n",
        "    # max_tokens=None,\n",
        "    # timeout=None,\n",
        "    # max_retries=2,\n",
        ")"
      ],
      "metadata": {
        "id": "EQx7GFjWOqso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create chain and invoke"
      ],
      "metadata": {
        "id": "b3QKZ-tlPLMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "response=rag_chain.invoke(user_query)"
      ],
      "metadata": {
        "id": "RqhGAa6wqkv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lajBKQJBRx-8",
        "outputId": "669354f5-4af9-4b48-f00c-8b09ac6586e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but the provided text does not explicitly state the three core parts of an agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the pipeline"
      ],
      "metadata": {
        "id": "A9ZswQfriusa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragas"
      ],
      "metadata": {
        "id": "9tYfuj5Yit8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import EvaluationDataset\n",
        "\n",
        "#sample queries and expected responses for the sample pdf files given\n",
        "# sample_queries = [\"What are the three core parts of an agent?\",\n",
        "#              \"Which hypothesis says a single reward can be enough for intelligence?\",\n",
        "#              \"Which system became the first AI to earn an IMO medal?\",\n",
        "#              \"What are the two basic multi-agent orchestration patterns?\",\n",
        "#              \"List the three steps in the guardrail-setup heuristic.\"\n",
        "#             ]\n",
        "\n",
        "# #If you are not interested in the context_recall metric, you don’t need to provide the ground_truths information.\n",
        "# expected_responses = [\"Model, Tools, Instructions\",\n",
        "#                 \"'Reward is Enough' hypothesis\",\n",
        "#                 \"AlphaProof\",\n",
        "#                 \"Manager pattern and Decentralised pattern\",\n",
        "#                 \"1 Focus on privacy & safety 2 Add guardrails for real-world edge cases 3 Tune for both security and user experience\"\n",
        "#                  ]\n",
        "\n",
        "# sample queries and expected responses for the sample website given\n",
        "sample_queries = [\"What three components sit alongside the LLM “brain” in an autonomous agent system?\",\n",
        "             \"What are the two main kinds of memory an agent maintains?\",\n",
        "             \"Which prompting method tells the model to 'think step by step'?\",\n",
        "             \"Which framework fuses reasoning traces with discrete actions inside an agent?\",\n",
        "             \"Generative Agents score memories on three factors; name them.\"\n",
        "            ]\n",
        "\n",
        "#If you are not interested in the context_recall metric, you don’t need to provide the ground_truths information.\n",
        "expected_responses = [\"Planning, Memory, Tool use\",\n",
        "                \"Short-term memory and Long-term memory\",\n",
        "                \"Chain of Thought (CoT)\",\n",
        "                \"ReAct\",\n",
        "                \"Recency, Importance, Relevance\"\n",
        "                 ]\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for query, reference in zip(sample_queries, expected_responses):\n",
        "    relevant_docs = retriever.invoke(query)\n",
        "    response = rag_chain.invoke(query)\n",
        "    dataset.append(\n",
        "        {\n",
        "            \"user_input\": query,\n",
        "            \"retrieved_contexts\": [rdoc.page_content for rdoc in relevant_docs],\n",
        "            \"response\": response,\n",
        "            \"reference\": reference,\n",
        "        }\n",
        "    )\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_list(dataset)"
      ],
      "metadata": {
        "id": "dg_Nx4Rebt5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
        "    llm=evaluator_llm,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "tltlkvNncyx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y0WAwE1c0IX",
        "outputId": "12346902-956e-4316-e4fd-cc5a94b7bf76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'context_recall': 0.2667, 'faithfulness': 0.5139, 'factual_correctness(mode=f1)': 0.4480}\n"
          ]
        }
      ]
    }
  ]
}